{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks with PyTorch\n",
    "\n",
    "Deep learning networks tend to be massive with dozens or hundreds of layers, that's where the term \"deep\" comes from. You can build one of these deep networks using only weight matrices as we did in the previous notebook, but in general it's very cumbersome and difficult to implement. PyTorch has a nice module `nn` that provides a nice way to efficiently build large neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we'll use the MNIST dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels, you can see a sample below\n",
    "\n",
    "<img src='assets/mnist.png'>\n",
    "\n",
    "Our goal is to build a neural network that can take one of these images and predict the digit in the image.\n",
    "\n",
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. Later, we'll use this to loop through the dataset for training, like\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "You'll notice I created the `trainloader` with a batch size of 64, and `shuffle=True`. The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a *batch*. And `shuffle=True` tells it to shuffle the dataset every time we start going through the data loader again. But here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size `(64, 1, 28, 28)`. So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what one of the images looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[1].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[1].numpy().squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fc819ce898>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to build a simple network for this dataset using weight matrices and matrix multiplications. Then, we'll see how to do it using PyTorch's `nn` module which provides a much more convenient and powerful method for defining network architectures.\n",
    "\n",
    "The networks you've seen so far are called *fully-connected* or *dense* networks. Each unit in one layer is connected to each unit in the next layer. In fully-connected networks, the input to each layer must be a one-dimensional vector (which can be stacked into a 2D tensor as a batch of multiple examples). However, our images are 28x28 2D tensors, so we need to convert them into 1D vectors. Thinking about sizes, we need to convert the batch of images with shape `(64, 1, 28, 28)` to a have a shape of `(64, 784)`, 784 is 28 times 28. This is typically called *flattening*, we flattened the 2D images into 1D vectors.\n",
    "\n",
    "Previously you built a network with one output unit. Here we need 10 output units, one for each digit. We want our network to predict the digit shown in an image, so what we'll do is calculate probabilities that the image is of any one digit or class. This ends up being a discrete probability distribution over the classes (digits) that tells us the most likely class for the image. That means we need 10 output units for the 10 classes (digits). We'll see how to convert the network output into a probability distribution next.\n",
    "\n",
    "> **Exercise:** Flatten the batch of images `images`. Then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we'll add one that gives us a probability distribution next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.view(images.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pixels = images.shape[1] * images.shape[2] * images.shape[3]\n",
    "num_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.reshape(images.shape[0], images.shape[1] * images.shape[2] * images.shape[3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, torch.Size([64, 784]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt = images.reshape(images.shape[0], images.shape[1] * images.shape[2] * images.shape[3])\n",
    "len(inpt), inpt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = images.view(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.eq(inpt,inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous notebook. s/b same as torch.sigmoid\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "??torch.sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docstring:\n",
    "sigmoid(input, out=None) -> Tensor\n",
    "\n",
    "Returns a new tensor with the sigmoid of the elements of :attr:`input`.\n",
    "\n",
    ".. math::\n",
    "    $\\text{out}_{i} = \\frac{1}{1 + e^{-\\text{input}_{i}}}$\n",
    "\n",
    "Args:\n",
    "    input (Tensor): the input tensor\n",
    "    out (Tensor, optional): the output tensor\n",
    "\n",
    "Example::\n",
    "\n",
    "    >>> a = torch.randn(4)\n",
    "    >>> a\n",
    "    tensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n",
    "    >>> torch.sigmoid(a)\n",
    "    tensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n",
    "Type:      builtin_function_or_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Sigmoid??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init signature: torch.nn.Sigmoid()\n",
    "Source:        \n",
    "class Sigmoid(Module):\n",
    "    r\"\"\"Applies the element-wise function \n",
    "    \n",
    "    \n",
    ":math:$\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}$\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Output: :math:`(N, *)`, same shape as the input\n",
    "\n",
    "    .. image:: scripts/activation_images/Sigmoid.png\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Sigmoid()\n",
    "        >>> input = torch.randn(2)\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.sigmoid(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-a067d074bdfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "torch.nn.Sigmoid(torch.randn(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-e3f460f7c953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2)\n",
    "torch.nn.Sigmoid(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to be instantiated..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2066, 0.7266])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Sigmoid()\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2066, 0.7266])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "testW = torch.randn(784, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9986e-01, 9.9993e-01, 1.0000e+00,  ..., 1.0000e+00, 3.2932e-12,\n",
       "         2.7598e-11],\n",
       "        [7.5980e-01, 1.0000e+00, 3.0711e-03,  ..., 1.0000e+00, 9.9977e-01,\n",
       "         4.4198e-10],\n",
       "        [6.2507e-09, 1.0000e+00, 9.9782e-01,  ..., 1.0000e+00, 1.1561e-31,\n",
       "         3.3801e-10],\n",
       "        ...,\n",
       "        [1.0000e+00, 1.0000e+00, 7.5269e-01,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         9.2114e-01],\n",
       "        [9.7628e-01, 7.7542e-01, 1.0000e+00,  ..., 1.0000e+00, 1.0910e-23,\n",
       "         5.1953e-13],\n",
       "        [1.0000e+00, 1.0000e+00, 9.9950e-01,  ..., 1.0000e+00, 9.5772e-04,\n",
       "         6.2056e-13]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation(torch.mm(inputs, testW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9986e-01, 9.9993e-01, 1.0000e+00,  ..., 1.0000e+00, 3.2932e-12,\n",
       "         2.7598e-11],\n",
       "        [7.5980e-01, 1.0000e+00, 3.0711e-03,  ..., 1.0000e+00, 9.9977e-01,\n",
       "         4.4198e-10],\n",
       "        [6.2507e-09, 1.0000e+00, 9.9782e-01,  ..., 1.0000e+00, 1.1561e-31,\n",
       "         3.3801e-10],\n",
       "        ...,\n",
       "        [1.0000e+00, 1.0000e+00, 7.5269e-01,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         9.2114e-01],\n",
       "        [9.7628e-01, 7.7542e-01, 1.0000e+00,  ..., 1.0000e+00, 1.0910e-23,\n",
       "         5.1953e-13],\n",
       "        [1.0000e+00, 1.0000e+00, 9.9950e-01,  ..., 1.0000e+00, 9.5772e-04,\n",
       "         6.2056e-13]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.mm(inputs, testW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, dtype=torch.uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(activation(torch.mm(inputs, testW)), torch.sigmoid(torch.mm(inputs, testW))).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(torch.sigmoid(torch.mm(inputs, testW)), -activation(torch.mm(inputs, testW)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "inpt = images.view(images.shape[0], -1)\n",
    "num_pixls = inpt.shape[1]\n",
    "W1 = torch.randn(num_pixls, 256)\n",
    "B1 = torch.randn(256)\n",
    "W2 = torch.randn(256, 10)\n",
    "B2 = torch.randn(10)\n",
    "\n",
    "h = activation(torch.mm(inpt, W1) + B1)\n",
    "\n",
    "#out = # output of your network, should have shape (64,10)\n",
    "out = torch.mm(h, W2) + B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9.0149,  -3.9841,  -2.2665,   6.9339,  10.9644,  -9.1111,  13.1147,\n",
       "         -10.0428,  19.4811,  -2.0752],\n",
       "        [ 13.0690,   2.8698,  -7.0353,   9.5743,   6.7962, -12.5223,  26.1957,\n",
       "          -6.7012,  13.8372,   4.0268],\n",
       "        [ 15.5708,   9.5194, -10.2859,  11.3327,   9.6821, -13.1002,  15.0776,\n",
       "          -5.5863,  11.7244, -15.1508],\n",
       "        [ 13.7821,   4.6612, -12.2135,  13.1703,   8.4305,  -5.6966,  24.2911,\n",
       "           1.2507,   3.6013,  -7.3416],\n",
       "        [ 18.7018,  -4.8383, -17.0509,  15.7735,  10.7959,  -4.4424,  16.1095,\n",
       "          -2.7217,   9.1579,   5.5480],\n",
       "        [ 19.0608,   6.0115, -14.4118,  10.2583,  14.9380,   6.9678,  21.8922,\n",
       "         -14.2682,   6.9539, -10.6708],\n",
       "        [ 14.3726,   0.0075, -11.7016,  14.7059,  10.4396,  -7.6697,  20.0365,\n",
       "           9.2143,   4.0143,  -1.2273],\n",
       "        [ 14.8708,   1.5581, -10.5830,  19.2161,  30.5141,  -3.8330,  12.2286,\n",
       "          -6.3575,  11.9110,  -0.3883],\n",
       "        [ 13.7596,   6.1246,  -9.3318,   5.7922,  16.3214, -10.4464,  17.8959,\n",
       "          -3.6598,  12.4312,   2.8694],\n",
       "        [ 17.8262,  -1.3921, -10.6382,  14.1284,  12.1382, -18.8750,  17.7566,\n",
       "           4.5461,   3.7432,  -2.4088],\n",
       "        [ 10.7892,   1.2574, -13.9023,   9.8179,  11.6485, -10.0285,  16.4739,\n",
       "           1.0792,  14.1192,   5.4295],\n",
       "        [  1.1916,  -0.5593,  -7.1859,  17.0348,   7.7578,  -8.6715,  13.5490,\n",
       "          11.2640,  10.5075,  -7.6012],\n",
       "        [ -2.2979,   0.9179, -10.6319,   9.5667,   6.1547, -17.2394,  29.1803,\n",
       "          -2.0822,   9.7898,  -4.1216],\n",
       "        [ 17.5345,   2.9211,  -8.2106,   4.3288,  10.2688,  -1.9566,  11.1023,\n",
       "          -6.9214,  14.7946,  -6.7314],\n",
       "        [ 17.8962,   9.7775, -14.1016,   9.3174,   5.1655,  -2.0161,  10.2127,\n",
       "           5.1758,  18.7200, -11.8098],\n",
       "        [  8.9297,   0.4887,  -3.6031,  16.0636,   1.4166,  -5.4094,  17.6372,\n",
       "           3.2874,  -1.1324,  -0.9017],\n",
       "        [ 12.1645,  -0.6491,  -8.3003,   6.2685,  12.3639,   0.7260,  23.6331,\n",
       "          -3.7088,  13.1408,   4.7413],\n",
       "        [ 16.9697,  -3.2707,  -5.8616,  16.1783,   5.8011, -14.7517,  13.5935,\n",
       "         -13.5326,  16.5089,  -2.3399],\n",
       "        [ 16.3151,   2.2624, -17.5378,  10.2045,   4.2205, -12.6204,  19.3328,\n",
       "          -0.7143,   1.3602,  -0.9765],\n",
       "        [ 25.2435,   6.4834, -13.6306,  12.4372,  15.1993,   3.2603,  14.4272,\n",
       "          -1.3545,  15.3521, -11.3009],\n",
       "        [ 14.5903,  -5.3035,  -5.5715,  12.6316,  13.4414,  -8.2854,  17.9007,\n",
       "           1.0620,  10.7350,  -8.3194],\n",
       "        [ 11.8649,   3.6794,  -4.9006,  16.3336,  18.9550,  -7.4570,  19.0930,\n",
       "           8.8483,   8.9375,  -6.5960],\n",
       "        [  8.3322,   2.6467,  -6.9567,  12.4152,  17.0505,  -2.4471,  21.0008,\n",
       "          -0.3567,   9.7767,  -1.2892],\n",
       "        [  8.8157,   3.2424, -18.2849,  13.3396,   8.3083, -10.6432,  10.8896,\n",
       "           5.3949,  11.2831,   7.4483],\n",
       "        [ 11.9764,   0.7129,   1.1287,  21.2835,   5.8814, -18.4114,  26.9363,\n",
       "          -9.5069,  12.0978,  -4.7570],\n",
       "        [ 19.4220,   6.1042,  -5.5381,  14.7579,  13.6531,   1.4109,  12.8629,\n",
       "           3.9093,   5.2323, -11.2207],\n",
       "        [  9.2414,   6.7196,  -7.4859,   9.0028,  13.9489,  -0.9291,  10.6538,\n",
       "           8.4755,   2.7132,   0.7205],\n",
       "        [ 13.6829,  -3.0037,  -7.4263,  17.0899,  11.0962, -17.6105,  21.1921,\n",
       "          -7.2774,  21.7517,   2.8428],\n",
       "        [ 15.8809,  14.4894,  -2.9064,   1.0605,  16.3928,  -1.0009,  16.8849,\n",
       "           4.4253,  15.1804, -17.8754],\n",
       "        [ 12.8267,  -7.7981,  -8.0727,  10.0866,   9.5019,  -7.4802,  20.6281,\n",
       "          -0.4790,   5.8848,   7.3705],\n",
       "        [ 13.9808,   1.8037,  -6.3736,   7.8300,  14.2806,  -9.5231,  25.5250,\n",
       "          -6.7478,   3.9293,   0.1784],\n",
       "        [ 19.1330,   8.0343, -12.8099,   8.6451,  16.4483,   0.5550,   9.6584,\n",
       "          -4.3617,   2.1831,  -3.3925],\n",
       "        [ 24.6298,   5.9343, -14.3660,   2.2380,   3.0642, -15.0584,   7.9175,\n",
       "          -0.6376,   6.7410, -12.2328],\n",
       "        [ 11.9240,  -1.0062, -11.2256,   8.3539,  -2.5777, -13.0343,   7.6015,\n",
       "          -3.0245,   9.1084,   6.0751],\n",
       "        [ 21.8800,  12.6845, -22.9506,   5.3126,   3.6835,   5.1530,   6.7025,\n",
       "          -5.5896,   7.4897,  -6.1642],\n",
       "        [  5.5110,  -7.6265, -12.3898,  11.5282,   5.5785, -16.8618,  13.9945,\n",
       "           4.5205,  12.7341,   4.4965],\n",
       "        [ 15.1116,  -0.4327,  -7.6107,  19.2848,   7.8374, -13.4152,  23.1806,\n",
       "          -1.5615,  16.6001,  -1.3564],\n",
       "        [ 12.6456,   5.8396, -15.2991,  10.2785,   2.5783, -14.3070,  14.4431,\n",
       "          -6.5405,  14.8895,   1.7728],\n",
       "        [  8.4898,   0.4854,  -6.2626,  12.9647,   8.3311,  -8.1436,  16.2408,\n",
       "          -2.0995,  16.1200,  -4.7151],\n",
       "        [  2.7729,   3.7446,  -8.6790,  12.2972,   8.1869,  -8.7765,  18.6655,\n",
       "           0.4987,  14.8603,  -1.2834],\n",
       "        [ 11.1631,   1.1517,  -7.6185,   5.5804,   2.3921, -16.2391,  13.7425,\n",
       "          -2.1851,  16.2934,  -0.4675],\n",
       "        [  7.6026,  -6.6773, -12.4610,  22.4000,  -0.1791,  -3.9952,  11.8895,\n",
       "           9.2078,   1.9655,  -4.1193],\n",
       "        [ 13.7007,   1.5046,  -9.9366,   9.1784,   3.3390,  -8.3924,  17.0981,\n",
       "           3.5384,  14.3361,  -1.3244],\n",
       "        [ 13.9740,  -1.7332,  -2.9062,  12.0136,  20.1323,  -6.0075,  23.7124,\n",
       "          -2.1915,  14.0299,  -3.3137],\n",
       "        [ 19.0783,   0.6296, -14.1415,   6.0863,   9.4034, -10.1168,  16.0103,\n",
       "          -3.9885,  16.0667,   3.2351],\n",
       "        [ 15.0919,  -0.0115,  -8.7417,  11.7370,  11.0005,  -3.3313,   7.7384,\n",
       "          -1.5444,  19.1940,   0.7187],\n",
       "        [ 18.9193,  -2.1448,  -6.0469,  10.0892,   7.8296,   1.2348,  13.1834,\n",
       "          -7.5597,   0.3049,  -3.1151],\n",
       "        [ 13.7366,  -3.4380, -13.9335,  15.2342,   0.1806, -17.8417,  15.5138,\n",
       "          -6.8383,   3.5744,   6.4116],\n",
       "        [ 16.4901,  -0.0432, -17.3391,  15.4963,  12.8358, -19.6933,  22.8152,\n",
       "          -3.6729,  10.4777, -10.6893],\n",
       "        [ 14.3809,   1.7883,  -2.5037,  11.8318,  14.0798, -16.8834,  24.1541,\n",
       "          -2.1514,   8.2518,  -1.3576],\n",
       "        [ 17.9408,   2.4631,  -6.6770,   9.7411,   9.2217,  -9.9725,  15.8878,\n",
       "          -5.0291,   7.1242,  -3.2401],\n",
       "        [ 14.2909,  -3.3105, -10.0128,  13.5676,  11.0041,  -1.4358,  17.4752,\n",
       "          11.7545,   8.0431,   6.2212],\n",
       "        [  3.9652,  -8.3424, -10.1690,   9.5978,  -1.4009,  -7.5340,  20.0521,\n",
       "           0.9020,   4.4395,  -2.3143],\n",
       "        [ 15.8315,   6.5857, -13.8960,   6.1403,   6.2894, -12.3491,  23.4767,\n",
       "          -4.8580,   4.5490,   7.7531],\n",
       "        [  9.6459,  -2.5855,  -6.0366,  13.1234,   9.8478, -11.9443,  20.1267,\n",
       "           1.3166,   3.1935,   5.8094],\n",
       "        [ 17.2260,   4.4170,  -9.4021,   9.1470,   9.0090,   3.8789,  16.7060,\n",
       "           1.3546,  -0.1672,  -6.1039],\n",
       "        [  2.7394,  -2.8167,  -0.8613,  14.6968,   8.4933, -13.5227,   9.9298,\n",
       "           4.8994,  14.3442,   4.9593],\n",
       "        [ 11.0485,   0.0398,   0.7096,  10.4608,   9.0138,  -5.7860,  19.1950,\n",
       "          -4.4486,   7.2893,  -4.1517],\n",
       "        [ 13.6898,   0.4971,  -5.8143,  13.4868,   7.6557,  -7.7274,  20.8919,\n",
       "           5.2704,  -4.6181,   8.9333],\n",
       "        [ 10.8738,   3.7466, -10.9676,  10.9222,   8.4102,  -3.6942,  17.6281,\n",
       "           8.1088,   3.1537,  -4.4568],\n",
       "        [ 22.9906,   2.2599, -11.7390,  11.1784,   9.0565, -14.5800,  10.1391,\n",
       "          -2.4969,   9.1372,   8.9864],\n",
       "        [ 11.1315,  -3.2782, -16.4276,  10.0353,   1.7067,  -7.5298,  23.2730,\n",
       "          -4.9980,  -0.1293,  -6.3106],\n",
       "        [ 20.9342,   0.3550,  -4.3892,  -0.2901,  20.5174,  -8.1945,  13.0827,\n",
       "         -21.3764,  17.0308,   9.4936],\n",
       "        [  2.0001,  -3.0347, -13.0579,  13.5883,  10.0458,  -5.7941,  19.0613,\n",
       "           8.1702,   5.4446,   4.1484]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 10 outputs for our network. We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to. Something that looks like this:\n",
    "<img src='assets/image_distribution.png' width=500px>\n",
    "\n",
    "Here we see that the probability for each class is roughly the same. This is representing an untrained network, it hasn't seen any data yet so it just returns a uniform distribution with equal probabilities for each class.\n",
    "\n",
    "To calculate this probability distribution, we often use the [**softmax** function](https://en.wikipedia.org/wiki/Softmax_function). Mathematically this looks like\n",
    "\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "\n",
    "What this does is squish each input $x_i$ between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one.\n",
    "\n",
    "> **Exercise:** Implement a function `softmax` that performs the softmax calculation and returns probability distributions for each example in the batch. Note that you'll need to pay attention to the shapes when doing this. If you have a tensor `a` with shape `(64, 10)` and a tensor `b` with shape `(64,)`, doing `a/b` will give you an error because PyTorch will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. The way to think about this is for each of the 64 examples, you only want to divide by one value, the sum in the denominator. So you need `b` to have a shape of `(64, 1)`. This way PyTorch will divide the 10 values in each row of `a` by the one value in each row of `b`. Pay attention to how you take the sum as well. You'll need to define the `dim` keyword in `torch.sum`. Setting `dim=0` takes the sum across the rows while `dim=1` takes the sum across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "??torch.nn.functional.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signature: torch.nn.functional.softmax(input, dim=None, _stacklevel=3)\n",
    "Source:   \n",
    "def softmax(input, dim=None, _stacklevel=3):\n",
    "    r\"\"\"Applies a softmax function.\n",
    "\n",
    "    Softmax is defined as:\n",
    "\n",
    ".. math::\n",
    "    $\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}$\n",
    "\n",
    "    This was sigmoid\n",
    "    \n",
    ".. math::\n",
    "    $\\text{out}_{i} = \\frac{1}{1 + e^{-\\text{input}_{i}}}$\n",
    " \n",
    "    It is applied to all slices along dim, and will re-scale them so that the elements\n",
    "    lie in the range `(0, 1)` and sum to 1.\n",
    "\n",
    "    See :class:`~torch.nn.Softmax` for more details.\n",
    "\n",
    "    Arguments:\n",
    "        input (Tensor): input\n",
    "        dim (int): A dimension along which softmax will be computed.\n",
    "\n",
    "    .. note::\n",
    "        This function doesn't work directly with NLLLoss,\n",
    "        which expects the Log to be computed between the Softmax and itself.\n",
    "        Use log_softmax instead (it's faster and has better numerical properties).\n",
    "\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = _get_softmax_dim('softmax', input.dim(), _stacklevel)\n",
    "    return input.softmax(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Softmax??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init signature: torch.nn.Softmax(dim=None)\n",
    "Source:        \n",
    "class Softmax(Module):\n",
    "    r\"\"\"Applies the Softmax function to an n-dimensional input Tensor\n",
    "    rescaling them so that the elements of the n-dimensional output Tensor\n",
    "    lie in the range (0,1) and sum to 1\n",
    "\n",
    "    Softmax is defined as\n",
    ":math:\n",
    "    $\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n",
    "\n",
    "    Shape:\n",
    "        - Input: any shape\n",
    "        - Output: same as input\n",
    "\n",
    "    Returns:\n",
    "        a Tensor of the same dimension and shape as the input with\n",
    "        values in the range [0, 1]\n",
    "\n",
    "    Arguments:\n",
    "        dim (int): A dimension along which Softmax will be computed (so every slice\n",
    "            along dim will sum to 1).\n",
    "\n",
    "    .. note::\n",
    "        This module doesn't work directly with NLLLoss,\n",
    "        which expects the Log to be computed between the Softmax and itself.\n",
    "        Use `LogSoftmax` instead (it's faster and has better numerical properties).\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Softmax()\n",
    "        >>> input = torch.randn(2, 3)\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=None):\n",
    "        super(Softmax, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        if not hasattr(self, 'dim'):\n",
    "            self.dim = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.softmax(input, self.dim, _stacklevel=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4754, 0.5246])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(2)\n",
    "torch.nn.functional.softmax(input, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-37b7d6706911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'dim'"
     ]
    }
   ],
   "source": [
    "torch.nn.Softmax(input, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4754, 0.5246])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-40316702c39a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "s(input, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to instantiate Softmax object with dim as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4754, 0.5246])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.nn.Softmax(dim=0)\n",
    "s(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]),\n",
       " tensor([[1.7173e-01, 2.7401e-02, 3.5016e+03, 2.2684e-06, 4.5525e+02, 2.0518e-01,\n",
       "          1.8035e+03, 1.5575e+00, 5.2676e+03, 1.5160e-01],\n",
       "         [2.6605e-01, 4.4978e+02, 1.7034e-05, 7.7574e-01, 8.9306e+02, 6.4978e+02,\n",
       "          2.6141e+01, 5.4677e-05, 2.2795e+02, 3.9863e+04],\n",
       "         [2.1525e+01, 1.3231e+00, 2.5557e-03, 3.0635e-08, 3.7510e+06, 8.7558e+03,\n",
       "          1.2282e+08, 2.2334e-04, 1.3056e+03, 4.0319e-02],\n",
       "         [1.1829e+00, 4.2110e-03, 2.2063e-01, 2.7705e-06, 2.8487e+02, 2.0386e-03,\n",
       "          2.4586e+06, 2.9571e-07, 2.6312e-03, 8.2609e+03],\n",
       "         [1.4204e-05, 1.0667e+00, 2.1048e-03, 1.0409e-01, 6.3954e+01, 5.4820e+01,\n",
       "          1.2355e+02, 2.4131e-03, 2.9515e+03, 8.8063e+04],\n",
       "         [5.5564e-02, 2.2440e-02, 9.3414e-08, 1.0062e-04, 2.5518e+02, 1.2782e+03,\n",
       "          1.0745e+05, 2.1609e-06, 1.5894e+04, 4.1460e+03],\n",
       "         [1.5987e-04, 8.3360e-02, 6.8709e-04, 1.3722e-03, 1.1685e+06, 1.2005e+01,\n",
       "          7.4790e+02, 7.9256e+01, 6.4086e+00, 2.6979e+03],\n",
       "         [8.1635e-06, 1.4250e-03, 6.0154e-06, 1.7354e-05, 5.1826e+05, 3.6051e+03,\n",
       "          2.3029e+02, 4.5532e-04, 1.4260e+04, 3.4746e+07],\n",
       "         [4.2390e-03, 4.0277e+00, 1.0033e+01, 6.5117e-05, 2.6241e+02, 1.4215e+03,\n",
       "          5.5361e+03, 5.8176e-04, 6.4981e-02, 1.0929e+05],\n",
       "         [2.8778e-02, 6.0529e-02, 7.8599e-03, 4.3083e-01, 6.6389e-01, 4.4880e+01,\n",
       "          1.8290e+07, 3.4888e-06, 2.0832e+02, 8.2616e-02],\n",
       "         [6.2575e-03, 9.7036e-01, 9.0382e+00, 5.7319e-06, 3.0760e-02, 2.4261e+01,\n",
       "          2.7470e+02, 8.4570e-05, 8.0836e+00, 2.4303e+04],\n",
       "         [7.0214e-07, 7.4825e-01, 6.8420e-03, 9.6268e-03, 1.1210e+06, 5.2807e-04,\n",
       "          6.5964e+06, 5.3348e+00, 6.4332e+00, 1.2507e+02],\n",
       "         [1.7308e+00, 1.5832e+03, 7.8002e-02, 5.0951e-01, 1.8148e+01, 1.6785e+00,\n",
       "          8.3795e+04, 7.8210e-08, 4.4949e+01, 5.5264e+02],\n",
       "         [3.0540e-02, 1.0167e-01, 7.2281e-01, 2.5382e-05, 3.3518e+02, 7.7163e-04,\n",
       "          1.9374e+05, 1.1330e-09, 7.8191e+02, 1.0294e+00],\n",
       "         [3.3781e-03, 6.1922e+00, 9.7297e-03, 3.7952e-08, 1.2114e+02, 1.5798e-01,\n",
       "          2.4126e+05, 1.1157e+00, 2.3899e+00, 4.6501e+02],\n",
       "         [2.8970e-04, 8.8038e-04, 1.1048e-03, 6.6399e-02, 7.3724e+00, 1.6849e+01,\n",
       "          1.7306e+00, 1.3937e-05, 1.6563e+01, 7.0011e+00],\n",
       "         [4.6228e+00, 6.2770e+00, 6.8137e-02, 1.2841e-01, 2.6919e-04, 2.7495e+02,\n",
       "          1.1642e+06, 2.7140e-03, 3.4588e-03, 4.5151e+06],\n",
       "         [1.1000e+00, 7.2263e-03, 1.6660e-02, 1.2186e-05, 1.6243e+05, 9.3284e-02,\n",
       "          4.7343e+06, 1.0414e+02, 7.9454e+01, 3.4099e-02],\n",
       "         [7.8181e-02, 1.3555e+00, 7.9153e-04, 3.3425e-04, 1.3465e+06, 2.5536e+00,\n",
       "          5.0355e+05, 6.2581e-06, 5.1940e-02, 3.3201e+07],\n",
       "         [1.1819e-04, 1.3619e+00, 3.6294e-04, 3.9865e-08, 5.7440e+03, 1.4393e-04,\n",
       "          9.2726e+05, 3.4289e-04, 5.4608e-02, 9.3762e+00],\n",
       "         [8.5570e-04, 4.9785e-03, 5.5361e-01, 4.4907e-05, 2.8333e-01, 4.2055e+01,\n",
       "          2.5010e+01, 5.1447e-04, 2.9908e-01, 7.7551e+00],\n",
       "         [2.3513e-02, 1.0601e+01, 6.3744e-02, 3.5871e-04, 2.0469e+04, 1.0176e-01,\n",
       "          1.2491e+06, 1.6755e-01, 1.4330e+03, 2.7865e+02],\n",
       "         [2.1145e-03, 1.1897e+03, 1.0172e-02, 5.2581e+00, 5.9315e+00, 1.3495e+03,\n",
       "          3.5343e+01, 7.9001e-08, 1.4902e+00, 5.1918e+05],\n",
       "         [1.0879e+00, 1.1335e-02, 5.1442e-08, 3.4240e-03, 3.8072e+02, 3.2767e+06,\n",
       "          4.7322e-02, 9.9172e-05, 5.0966e-01, 1.1563e+01],\n",
       "         [2.6044e-02, 6.4535e-02, 3.2725e-05, 5.6011e-01, 1.1822e-02, 4.6680e+05,\n",
       "          2.8799e+03, 4.6504e-08, 6.0942e+05, 6.8268e+04],\n",
       "         [9.8839e+00, 2.1251e+00, 6.2974e-06, 3.2189e-07, 1.6848e+05, 6.8930e+06,\n",
       "          2.1641e+07, 1.2459e-06, 1.0672e+03, 2.5594e-03],\n",
       "         [1.5392e-03, 4.9571e+00, 9.4602e+00, 5.1060e-07, 6.1394e+06, 6.9527e-03,\n",
       "          1.3512e+07, 4.0069e-04, 8.4181e-02, 2.2127e+02],\n",
       "         [3.7086e+01, 9.1032e+00, 1.4039e-02, 8.6234e+01, 2.2551e+05, 9.0095e+00,\n",
       "          7.8098e+04, 1.0706e-04, 7.6232e-02, 3.0430e+01],\n",
       "         [2.6256e+01, 3.8554e-02, 1.6292e-05, 7.5781e-08, 1.3588e+06, 3.5578e+00,\n",
       "          8.8253e+03, 1.2222e-03, 2.7306e+00, 3.3581e+00],\n",
       "         [2.8692e-03, 1.6400e+01, 4.9216e-03, 1.7382e-02, 2.6949e+00, 6.7170e-04,\n",
       "          2.9535e+04, 2.2850e-05, 1.5551e-03, 4.7422e+04],\n",
       "         [9.0953e-05, 1.7058e+00, 7.4040e-04, 2.1545e-02, 5.1718e-01, 6.8269e+05,\n",
       "          1.1404e+01, 2.5062e-07, 4.1141e+00, 1.0413e+03],\n",
       "         [1.0523e-03, 1.3115e+01, 9.9654e-02, 3.6899e-04, 3.0637e+09, 1.6892e-03,\n",
       "          2.9147e+02, 8.9308e-06, 1.3490e-03, 6.2717e+04],\n",
       "         [4.9182e-01, 3.8104e+00, 1.8390e-07, 6.8137e-07, 3.3300e+01, 1.3488e+03,\n",
       "          2.2259e+02, 3.5887e-05, 4.5553e+02, 2.9706e-03],\n",
       "         [1.3850e-03, 1.0570e-03, 2.3227e-01, 1.1676e-03, 2.2904e+05, 6.2918e+00,\n",
       "          4.4843e+02, 1.0754e-05, 7.1313e+02, 8.5591e+04],\n",
       "         [1.3299e-03, 1.0342e-01, 6.7408e-03, 9.5107e-09, 3.4892e+06, 1.0044e+00,\n",
       "          1.0702e+07, 1.2582e+01, 6.2772e-01, 1.5599e+00],\n",
       "         [1.8137e-02, 1.4545e+01, 1.1431e-07, 5.4267e-02, 8.5803e+01, 5.3659e+03,\n",
       "          1.4816e+02, 2.1708e-06, 7.5914e-03, 5.7265e+03],\n",
       "         [2.4997e-03, 2.5496e-02, 1.4511e+03, 9.8612e-05, 5.8360e+02, 6.4791e+00,\n",
       "          1.4516e+03, 1.4089e-02, 9.1100e+04, 1.8336e-04],\n",
       "         [1.8819e-02, 6.2669e+00, 1.1125e-04, 4.5697e-02, 4.4942e+02, 1.3809e-02,\n",
       "          7.8648e+02, 9.7991e-06, 2.6645e+02, 2.6923e+02],\n",
       "         [3.3799e-03, 3.6269e+01, 4.5136e-02, 1.6773e-02, 1.8534e+03, 4.4105e+02,\n",
       "          3.2843e+03, 2.1683e-06, 3.8129e+00, 7.3652e+02],\n",
       "         [8.4353e-05, 9.3772e+00, 1.1457e-02, 1.3988e+01, 5.3809e+01, 8.3274e+06,\n",
       "          3.6484e+01, 1.6074e-04, 4.5152e+01, 1.6850e+05],\n",
       "         [5.5147e-02, 5.2471e+00, 1.0933e-02, 9.2217e-04, 6.1020e+06, 4.3181e+01,\n",
       "          2.3015e-01, 2.7489e-05, 7.1174e+01, 3.5522e+02],\n",
       "         [3.0062e-02, 7.2254e-03, 9.8410e-06, 5.8381e-02, 7.8384e+02, 1.5033e+07,\n",
       "          4.7806e+02, 2.8233e+01, 2.2696e+03, 1.3500e+00],\n",
       "         [2.7605e-01, 2.6262e+04, 4.2665e-05, 2.0443e-04, 5.3661e+03, 3.1225e+01,\n",
       "          4.3026e+01, 6.9528e-07, 2.2495e-02, 9.2945e+05],\n",
       "         [5.5751e+00, 6.5499e-01, 1.0803e+00, 3.4460e-04, 2.9493e+06, 4.2411e+01,\n",
       "          1.9686e+03, 1.7924e-07, 7.5564e-02, 1.1846e+04],\n",
       "         [6.8256e-02, 5.8471e+03, 1.5296e-01, 1.5250e-04, 1.3372e+05, 1.5520e+00,\n",
       "          3.3429e+04, 9.7225e-06, 2.2361e-02, 2.4233e+05],\n",
       "         [4.3647e-06, 7.0307e-01, 3.8553e-02, 5.4964e-05, 1.6029e+00, 8.3978e-01,\n",
       "          1.1376e+06, 5.7166e-06, 7.3819e-03, 1.8028e+01],\n",
       "         [6.9819e+00, 1.0907e+01, 5.5090e-01, 7.6634e-08, 1.6643e+06, 1.7224e+01,\n",
       "          1.2028e+06, 1.3158e-06, 8.4181e+04, 1.4955e-03],\n",
       "         [1.6977e-01, 5.3341e+02, 1.9661e-04, 1.6614e+00, 1.5019e+01, 1.4316e+03,\n",
       "          2.0970e+03, 1.6605e-02, 9.7909e+04, 4.5889e+03],\n",
       "         [2.6841e+00, 2.7263e+04, 1.5869e+01, 2.0032e-05, 3.6954e+04, 9.8985e-04,\n",
       "          9.6775e+06, 6.8172e-05, 6.4781e-05, 4.5271e+00],\n",
       "         [7.4204e+00, 3.7547e+00, 7.2991e-03, 9.9955e-06, 8.8591e-01, 1.1924e+03,\n",
       "          5.7352e+00, 2.4812e-05, 2.3462e+00, 1.9052e+05],\n",
       "         [7.2429e-02, 7.0475e+00, 5.3817e-02, 1.8252e-04, 1.7895e-01, 8.4279e-01,\n",
       "          1.2141e+04, 1.2789e-06, 3.1338e+04, 1.2661e+02],\n",
       "         [1.3349e+00, 1.5618e-03, 3.7686e-04, 3.3434e-05, 3.6152e+01, 9.0914e-01,\n",
       "          4.2770e+00, 1.5522e-04, 6.1183e+03, 1.1396e+04],\n",
       "         [9.0172e-04, 4.3293e+01, 2.5193e-02, 2.9186e-02, 7.8810e+02, 2.0845e-01,\n",
       "          1.9042e+03, 3.0995e-03, 3.5810e+02, 3.4755e+05],\n",
       "         [3.2485e-01, 2.7015e+01, 4.2726e-03, 4.0746e-01, 4.2951e+00, 4.4770e-03,\n",
       "          2.8222e+03, 4.5757e-05, 9.8924e-01, 3.0823e+04],\n",
       "         [4.7664e-02, 1.2579e+00, 9.9733e-05, 1.5827e-03, 4.3350e+00, 2.0071e-03,\n",
       "          4.2815e+03, 2.2568e-04, 6.7200e-04, 2.5288e+03],\n",
       "         [8.6477e-03, 9.6501e-01, 1.2047e-03, 3.1345e-09, 1.1284e+04, 7.0334e+02,\n",
       "          9.6912e+01, 4.9429e-08, 4.1756e+05, 2.2213e-02],\n",
       "         [2.1402e-04, 9.8268e-01, 6.9746e-04, 4.6644e-01, 7.6367e+01, 3.0424e+02,\n",
       "          5.4917e-01, 1.9201e-02, 1.7366e+01, 1.9706e+01],\n",
       "         [5.6818e-06, 2.8425e-07, 8.3478e-03, 6.6023e-04, 2.9479e-01, 5.9138e+02,\n",
       "          7.2420e+02, 4.4931e-07, 6.7697e+06, 1.6632e+02],\n",
       "         [2.3123e-04, 1.3775e-01, 1.9113e-06, 3.2200e-03, 3.9932e+00, 2.2204e+00,\n",
       "          7.0610e+00, 1.9376e-05, 3.3055e+00, 1.0234e+05],\n",
       "         [4.0129e-04, 1.4321e+02, 3.0395e+00, 4.2916e-04, 6.5115e+02, 5.2608e-01,\n",
       "          1.2582e+07, 1.0798e-04, 5.4267e-01, 5.8525e+03],\n",
       "         [1.7589e-05, 9.1367e+00, 1.1503e-04, 2.1162e-06, 1.2408e+06, 3.4342e+02,\n",
       "          7.3310e+04, 1.4717e-04, 1.8465e+01, 1.9717e+01],\n",
       "         [3.4650e-03, 7.2746e-01, 6.5445e-03, 1.3299e-01, 8.1057e-04, 2.1231e+05,\n",
       "          9.5650e+02, 1.3504e-07, 1.2918e+03, 1.2319e-01],\n",
       "         [4.1432e-02, 1.1733e-03, 1.1763e+02, 1.4654e-08, 1.7673e+05, 3.3180e+00,\n",
       "          6.5079e+03, 1.6459e-03, 5.6252e-01, 2.3466e+02],\n",
       "         [6.0220e+00, 2.5935e+01, 3.5099e-01, 3.1525e-06, 4.9865e+06, 5.6560e+00,\n",
       "          2.4745e+06, 2.5439e-07, 7.8998e-01, 1.4082e+09]]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(out).shape, torch.exp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11030.1484), tensor(42110.5781), tensor(126582416.))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(out)[0].sum(), torch.exp(out)[1].sum(), torch.exp(out)[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]),\n",
       " tensor([1.1030e+04, 4.2111e+04, 1.2658e+08, 2.4671e+06, 9.1258e+04, 1.2903e+05,\n",
       "         1.1721e+06, 3.5282e+07, 1.1652e+05, 1.8291e+07, 2.4620e+04, 7.7176e+06,\n",
       "         8.5998e+04, 1.9486e+05, 2.4186e+05, 4.9586e+01, 5.6796e+06, 4.8969e+06,\n",
       "         3.5051e+07, 9.3302e+05, 7.5963e+01, 1.2712e+06, 5.2177e+05, 3.2771e+06,\n",
       "         1.1474e+06, 2.8703e+07, 1.9651e+07, 3.0378e+05, 1.3677e+06, 7.6976e+04,\n",
       "         6.8375e+05, 3.0637e+09, 2.0645e+03, 3.1580e+05, 1.4191e+07, 1.1341e+04,\n",
       "         9.4593e+04, 1.7779e+03, 6.3554e+03, 8.4960e+06, 6.1024e+06, 1.5036e+07,\n",
       "         9.6115e+05, 2.9631e+06, 4.1532e+05, 1.1376e+06, 2.9512e+06, 1.0658e+05,\n",
       "         9.7417e+06, 1.9173e+05, 4.3613e+04, 1.7556e+04, 3.5064e+05, 3.3678e+04,\n",
       "         6.8159e+03, 4.2965e+05, 4.1970e+02, 6.7712e+06, 1.0236e+05, 1.2588e+07,\n",
       "         1.3145e+06, 2.1456e+05, 1.8359e+05, 1.4156e+09]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.exp(out), dim=1).shape, torch.sum(torch.exp(out), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n",
      "tensor([[1.1030e+04],\n",
      "        [4.2111e+04],\n",
      "        [1.2658e+08],\n",
      "        [2.4671e+06],\n",
      "        [9.1258e+04],\n",
      "        [1.2903e+05],\n",
      "        [1.1721e+06],\n",
      "        [3.5282e+07],\n",
      "        [1.1652e+05],\n",
      "        [1.8291e+07],\n",
      "        [2.4620e+04],\n",
      "        [7.7176e+06],\n",
      "        [8.5998e+04],\n",
      "        [1.9486e+05],\n",
      "        [2.4186e+05],\n",
      "        [4.9586e+01],\n",
      "        [5.6796e+06],\n",
      "        [4.8969e+06],\n",
      "        [3.5051e+07],\n",
      "        [9.3302e+05],\n",
      "        [7.5963e+01],\n",
      "        [1.2712e+06],\n",
      "        [5.2177e+05],\n",
      "        [3.2771e+06],\n",
      "        [1.1474e+06],\n",
      "        [2.8703e+07],\n",
      "        [1.9651e+07],\n",
      "        [3.0378e+05],\n",
      "        [1.3677e+06],\n",
      "        [7.6976e+04],\n",
      "        [6.8375e+05],\n",
      "        [3.0637e+09],\n",
      "        [2.0645e+03],\n",
      "        [3.1580e+05],\n",
      "        [1.4191e+07],\n",
      "        [1.1341e+04],\n",
      "        [9.4593e+04],\n",
      "        [1.7779e+03],\n",
      "        [6.3554e+03],\n",
      "        [8.4960e+06],\n",
      "        [6.1024e+06],\n",
      "        [1.5036e+07],\n",
      "        [9.6115e+05],\n",
      "        [2.9631e+06],\n",
      "        [4.1532e+05],\n",
      "        [1.1376e+06],\n",
      "        [2.9512e+06],\n",
      "        [1.0658e+05],\n",
      "        [9.7417e+06],\n",
      "        [1.9173e+05],\n",
      "        [4.3613e+04],\n",
      "        [1.7556e+04],\n",
      "        [3.5064e+05],\n",
      "        [3.3678e+04],\n",
      "        [6.8159e+03],\n",
      "        [4.2965e+05],\n",
      "        [4.1970e+02],\n",
      "        [6.7712e+06],\n",
      "        [1.0236e+05],\n",
      "        [1.2588e+07],\n",
      "        [1.3145e+06],\n",
      "        [2.1456e+05],\n",
      "        [1.8359e+05],\n",
      "        [1.4156e+09]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(torch.exp(out), dim=1).view(-1, 1).shape)\n",
    "print(torch.sum(torch.exp(out), dim=1).view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "probtest = torch.exp(out) / torch.sum(torch.exp(out), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(probtest.shape)\n",
    "print()\n",
    "print(probtest.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "# Here, out should be the output of the network in the previous excercise with shape (64,10)\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "print()\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "prob1 = torch.nn.functional.softmax(out, dim=1)\n",
    "\n",
    "print(prob1.shape)\n",
    "print()\n",
    "print(prob1.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "s = torch.nn.Softmax(dim=1)\n",
    "\n",
    "prob2 = s(out)\n",
    "\n",
    "print(prob2.shape)\n",
    "print()\n",
    "print(prob2.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "PyTorch provides a module `nn` that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units, 10 output units and a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this bit by bit.\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Here we're inheriting from `nn.Module`. Combined with `super().__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "```\n",
    "\n",
    "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to `self.hidden`. The module automatically creates the weight and bias tensors which we'll use in the `forward` method. You can access the weight and bias tensors once the network once it's created with `net.hidden.weight` and `net.hidden.bias`.\n",
    "\n",
    "```python\n",
    "self.output = nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n",
    "\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting `dim=1` in `nn.Softmax(dim=1)` calculates softmax across the columns.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n",
    "\n",
    "```python\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "Here the input tensor `x` is passed through each operation and reassigned to `x`. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the `__init__` method doesn't matter, but you'll need to sequence the operations correctly in the `forward` method.\n",
    "\n",
    "Now we can create a `Network` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network and look at it's text representation\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define the network somewhat more concisely and clearly using the `torch.nn.functional` module. This is the most common way you'll see networks defined as many operations are simple element-wise functions. We normally import this module as `F`, `import torch.nn.functional as F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "So far we've only been looking at the softmax activation, but in general any function can be used as an activation function. The only requirement is that for a network to approximate a non-linear function, the activation functions must be non-linear. Here are a few more examples of common activation functions: Tanh (hyperbolic tangent), and ReLU (rectified linear unit).\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "In practice, the ReLU function is used almost exclusively as the activation function for hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn to Build a Network\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "> **Exercise:** Create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the `nn.ReLU` module or `F.relu` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Your solution here\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNIST_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden1 = nn.Linear(784, 128)\n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with sigmoid activation\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))    \n",
    "        \n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_Classifier(\n",
      "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MNIST_Classifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1           [-1, 64, 1, 128]         100,480\n",
      "            Linear-2            [-1, 64, 1, 64]           8,256\n",
      "            Linear-3            [-1, 64, 1, 10]             650\n",
      "================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 0.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(64, 1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0138, -0.0140, -0.0163,  ..., -0.0315,  0.0341, -0.0116],\n",
      "        [ 0.0027,  0.0292,  0.0157,  ..., -0.0356, -0.0225, -0.0297],\n",
      "        [ 0.0135, -0.0213,  0.0290,  ...,  0.0041, -0.0307,  0.0228],\n",
      "        ...,\n",
      "        [ 0.0070, -0.0198, -0.0177,  ...,  0.0031, -0.0127,  0.0123],\n",
      "        [-0.0190,  0.0281,  0.0176,  ..., -0.0060, -0.0348,  0.0308],\n",
      "        [ 0.0203, -0.0222, -0.0162,  ..., -0.0242, -0.0241, -0.0235]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.hidden1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0021,  0.0048,  0.0069,  0.0128, -0.0148,  0.0293,  0.0107,  0.0128,\n",
      "        -0.0101, -0.0300,  0.0187, -0.0155,  0.0107,  0.0028,  0.0121, -0.0138,\n",
      "        -0.0233,  0.0346,  0.0122, -0.0207,  0.0293, -0.0234, -0.0337,  0.0347,\n",
      "         0.0280,  0.0254, -0.0144,  0.0224, -0.0008,  0.0128, -0.0121, -0.0197,\n",
      "         0.0097,  0.0052, -0.0328, -0.0345,  0.0171, -0.0094,  0.0058,  0.0160,\n",
      "         0.0353,  0.0287,  0.0275,  0.0128, -0.0063,  0.0287, -0.0028,  0.0127,\n",
      "        -0.0318, -0.0325, -0.0288, -0.0083, -0.0328, -0.0149, -0.0352, -0.0076,\n",
      "        -0.0103, -0.0240,  0.0297, -0.0100, -0.0071,  0.0149,  0.0272,  0.0074,\n",
      "        -0.0014, -0.0030,  0.0108,  0.0199, -0.0112,  0.0054,  0.0278, -0.0341,\n",
      "         0.0181, -0.0195,  0.0350,  0.0085, -0.0095,  0.0026, -0.0013,  0.0139,\n",
      "         0.0064, -0.0269,  0.0211,  0.0049,  0.0098,  0.0250, -0.0038,  0.0140,\n",
      "         0.0070,  0.0336, -0.0346, -0.0260, -0.0195,  0.0004,  0.0196,  0.0126,\n",
      "         0.0120, -0.0297, -0.0030, -0.0045, -0.0166,  0.0005,  0.0234,  0.0344,\n",
      "         0.0254,  0.0115,  0.0108,  0.0251, -0.0137,  0.0106, -0.0162,  0.0135,\n",
      "         0.0309,  0.0100,  0.0264, -0.0139, -0.0216, -0.0084, -0.0311,  0.0287,\n",
      "        -0.0100,  0.0071,  0.0150,  0.0313,  0.0170,  0.0002,  0.0290, -0.0347],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.hidden1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Solution\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 128, 64, 10 units each\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0021, -0.0038, -0.0029,  ...,  0.0111, -0.0287, -0.0216],\n",
      "        [ 0.0300,  0.0225, -0.0021,  ...,  0.0144, -0.0273,  0.0277],\n",
      "        [ 0.0302,  0.0052,  0.0014,  ..., -0.0187,  0.0306, -0.0134],\n",
      "        ...,\n",
      "        [-0.0060,  0.0238, -0.0058,  ...,  0.0079, -0.0238, -0.0351],\n",
      "        [ 0.0045, -0.0248, -0.0213,  ..., -0.0051,  0.0346, -0.0328],\n",
      "        [ 0.0331,  0.0264,  0.0209,  ...,  0.0253,  0.0022,  0.0202]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0295,  0.0188, -0.0040,  0.0129, -0.0220,  0.0037, -0.0273, -0.0256,\n",
      "        -0.0063,  0.0130,  0.0274, -0.0008,  0.0319,  0.0188, -0.0185,  0.0253,\n",
      "        -0.0047, -0.0253,  0.0007, -0.0127,  0.0276,  0.0184,  0.0090, -0.0140,\n",
      "        -0.0187,  0.0124, -0.0137, -0.0346, -0.0259, -0.0134,  0.0084,  0.0262,\n",
      "        -0.0349,  0.0129, -0.0298, -0.0328, -0.0273, -0.0200, -0.0310,  0.0084,\n",
      "        -0.0192,  0.0206,  0.0052,  0.0119,  0.0244, -0.0062, -0.0210,  0.0302,\n",
      "        -0.0020,  0.0086,  0.0134,  0.0326,  0.0225,  0.0239,  0.0331, -0.0036,\n",
      "         0.0158, -0.0281, -0.0135, -0.0206,  0.0331,  0.0043,  0.0134, -0.0194,\n",
      "         0.0003,  0.0089,  0.0299, -0.0014,  0.0215,  0.0006, -0.0018, -0.0206,\n",
      "         0.0056,  0.0034, -0.0175, -0.0154, -0.0078, -0.0126,  0.0212, -0.0167,\n",
      "         0.0250,  0.0168, -0.0056,  0.0351,  0.0251,  0.0038, -0.0205,  0.0212,\n",
      "         0.0266, -0.0220, -0.0016,  0.0075,  0.0230, -0.0348, -0.0148,  0.0322,\n",
      "         0.0100, -0.0136,  0.0000, -0.0235, -0.0005,  0.0005,  0.0216,  0.0268,\n",
      "        -0.0270,  0.0297, -0.0347,  0.0292, -0.0083, -0.0292, -0.0241,  0.0087,\n",
      "         0.0342,  0.0184,  0.0146,  0.0302, -0.0331, -0.0061,  0.0216,  0.0227,\n",
      "         0.0154,  0.0099,  0.0341, -0.0355, -0.0184, -0.0319,  0.0221,  0.0210],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0003,  0.0072,  0.0023,  ..., -0.0016, -0.0061,  0.0003],\n",
       "        [-0.0112,  0.0027,  0.0183,  ...,  0.0196, -0.0123,  0.0033],\n",
       "        [-0.0048, -0.0074, -0.0071,  ..., -0.0092, -0.0132, -0.0059],\n",
       "        ...,\n",
       "        [ 0.0013, -0.0004,  0.0021,  ..., -0.0074, -0.0015,  0.0147],\n",
       "        [ 0.0071, -0.0364, -0.0042,  ..., -0.0048, -0.0045, -0.0017],\n",
       "        [ 0.0036, -0.0112, -0.0243,  ...,  0.0028,  0.0015, -0.0092]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fc8a897c50>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 224,
       "width": 423
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!\n",
    "\n",
    "### Using `nn.Sequential`\n",
    "\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fc8b81edd8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 224,
       "width": 423
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations are availble by passing in the appropriate index. For example, if you want to get first Linear operation and look at the weights, you'd use `model[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0253, -0.0009,  0.0138,  ...,  0.0283, -0.0175, -0.0276],\n",
       "        [-0.0259,  0.0339, -0.0080,  ...,  0.0210,  0.0133, -0.0228],\n",
       "        [ 0.0294,  0.0022, -0.0138,  ..., -0.0006, -0.0245, -0.0214],\n",
       "        ...,\n",
       "        [ 0.0153, -0.0274, -0.0094,  ...,  0.0277,  0.0098,  0.0356],\n",
       "        [-0.0096,  0.0119, -0.0258,  ...,  0.0044,  0.0145,  0.0164],\n",
       "        [-0.0067,  0.0171, -0.0299,  ...,  0.0058,  0.0260, -0.0146]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model[0])\n",
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can access layers either by integer or the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n",
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model[0])\n",
    "print(model.fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we'll see how we can train a neural network to accuractly predict the numbers appearing in the MNIST images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
